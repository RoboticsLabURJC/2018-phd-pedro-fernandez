{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_4_Cliff_walking(SARSA vs QLearning).ipynb","provenance":[],"authorship_tag":"ABX9TyMxgrfGecmT8HMiKbgGp8do"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Ie8hBk581qZL","executionInfo":{"status":"ok","timestamp":1612187378942,"user_tz":300,"elapsed":683,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["import numpy as np\n","import matplotlib\n","#matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QN9wQL12RL_","executionInfo":{"status":"ok","timestamp":1612187378944,"user_tz":300,"elapsed":668,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["# world height\n","WORLD_HEIGHT = 4\n","\n","# world width\n","WORLD_WIDTH = 12\n","\n","# probability for exploration\n","EPSILON = 0.1\n","\n","# step size\n","ALPHA = 0.5\n","\n","# gamma for Q-Learning and Expected Sarsa\n","GAMMA = 1\n","\n","# all possible actions\n","ACTION_UP = 0\n","ACTION_DOWN = 1\n","ACTION_LEFT = 2\n","ACTION_RIGHT = 3\n","ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n","\n","# initial state action pair values\n","START = [3, 0]\n","GOAL = [3, 11]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk52ydGB2RFc","executionInfo":{"status":"ok","timestamp":1612187378945,"user_tz":300,"elapsed":661,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["def step(state, action):\n","    i, j = state\n","    if action == ACTION_UP:\n","        next_state = [max(i - 1, 0), j]\n","    elif action == ACTION_LEFT:\n","        next_state = [i, max(j - 1, 0)]\n","    elif action == ACTION_RIGHT:\n","        next_state = [i, min(j + 1, WORLD_WIDTH - 1)]\n","    elif action == ACTION_DOWN:\n","        next_state = [min(i + 1, WORLD_HEIGHT - 1), j]\n","    else:\n","        assert False\n","\n","    reward = -1\n","    if (action == ACTION_DOWN and i == 2 and 1 <= j <= 10) or (\n","        action == ACTION_RIGHT and state == START):\n","        reward = -100\n","        next_state = START\n","\n","    return next_state, reward"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLLY_C6Z2Q97","executionInfo":{"status":"ok","timestamp":1612187379957,"user_tz":300,"elapsed":617,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["# choose an action based on epsilon greedy algorithm\n","def choose_action(state, q_value):\n","    if np.random.binomial(1, EPSILON) == 1:\n","        return np.random.choice(ACTIONS)\n","    else:\n","        values_ = q_value[state[0], state[1], :]\n","        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7FGrqjo28fz","executionInfo":{"status":"ok","timestamp":1612187445067,"user_tz":300,"elapsed":508,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["# an episode with Sarsa\n","# @q_value: values for state action pair, will be updated\n","# @expected: if True, will use expected Sarsa algorithm\n","# @step_size: step size for updating\n","# @return: total rewards within this episode\n","def sarsa(q_value, expected=False, step_size=ALPHA):\n","    state = START\n","    action = choose_action(state, q_value)\n","    rewards = 0.0\n","    while state != GOAL:\n","        next_state, reward = step(state, action)\n","        next_action = choose_action(next_state, q_value)\n","        rewards += reward\n","        if not expected:\n","            target = q_value[next_state[0], next_state[1], next_action]\n","        else:\n","            # calculate the expected value of new state\n","            target = 0.0\n","            q_next = q_value[next_state[0], next_state[1], :]\n","            best_actions = np.argwhere(q_next == np.max(q_next))\n","            for action_ in ACTIONS:\n","                if action_ in best_actions:\n","                    target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[0], next_state[1], action_]\n","                else:\n","                    target += EPSILON / len(ACTIONS) * q_value[next_state[0], next_state[1], action_]\n","        target *= GAMMA\n","        q_value[state[0], state[1], action] += step_size * (\n","                reward + target - q_value[state[0], state[1], action])\n","        state = next_state\n","        action = next_action\n","    return rewards"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_h-RRPU3AF_","executionInfo":{"status":"ok","timestamp":1612187441900,"user_tz":300,"elapsed":624,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["# an episode with Q-Learning\n","# @q_value: values for state action pair, will be updated\n","# @step_size: step size for updating\n","# @return: total rewards within this episode\n","def q_learning(q_value, step_size=ALPHA):\n","    state = START\n","    rewards = 0.0\n","    while state != GOAL:\n","        action = choose_action(state, q_value)\n","        next_state, reward = step(state, action)\n","        rewards += reward\n","        # Q-Learning update\n","        q_value[state[0], state[1], action] += step_size * (\n","                reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) -\n","                q_value[state[0], state[1], action])\n","        state = next_state\n","    return rewards"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"DwLb2lAo3XGs","executionInfo":{"status":"ok","timestamp":1612187533395,"user_tz":300,"elapsed":531,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}}},"source":["# print optimal policy\n","def print_optimal_policy(q_value):\n","    optimal_policy = []\n","    for i in range(0, WORLD_HEIGHT):\n","        optimal_policy.append([])\n","        for j in range(0, WORLD_WIDTH):\n","            if [i, j] == GOAL:\n","                optimal_policy[-1].append('G')\n","                continue\n","            bestAction = np.argmax(q_value[i, j, :])\n","            if bestAction == ACTION_UP:\n","                optimal_policy[-1].append('U')\n","            elif bestAction == ACTION_DOWN:\n","                optimal_policy[-1].append('D')\n","            elif bestAction == ACTION_LEFT:\n","                optimal_policy[-1].append('L')\n","            elif bestAction == ACTION_RIGHT:\n","                optimal_policy[-1].append('R')\n","    for row in optimal_policy:\n","        print(row)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZ-cRZNT2Q06","executionInfo":{"status":"ok","timestamp":1612187593819,"user_tz":300,"elapsed":50574,"user":{"displayName":"Pedro Fernández de Cabo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeGRStP0VEhwfus8Iy8fiAgXrKeQKS4F1Uj_nyI5c=s64","userId":"11364016381284516025"}},"outputId":"396e06cb-0106-4d84-be14-ccc19a97e3fb"},"source":["if __name__ == '__main__':\n","\n","    # episodes of each run\n","    episodes = 500\n","\n","    # perform 40 independent runs\n","    runs = 50\n","\n","    rewards_sarsa = np.zeros(episodes)\n","    rewards_q_learning = np.zeros(episodes)\n","    for r in tqdm(range(runs)):\n","        q_sarsa = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n","        q_q_learning = np.copy(q_sarsa)\n","        for i in range(0, episodes):\n","            # cut off the value by -100 to draw the figure more elegantly\n","            # rewards_sarsa[i] += max(sarsa(q_sarsa), -100)\n","            # rewards_q_learning[i] += max(q_learning(q_q_learning), -100)\n","            rewards_sarsa[i] += sarsa(q_sarsa)\n","            rewards_q_learning[i] += q_learning(q_q_learning)\n","\n","    # averaging over independt runs\n","    rewards_sarsa /= runs\n","    rewards_q_learning /= runs\n","\n","    # draw reward curves\n","    plt.plot(rewards_sarsa, label='Sarsa')\n","    plt.plot(rewards_q_learning, label='Q-Learning')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Sum of rewards during episode')\n","    plt.ylim([-100, 0])\n","    plt.legend()\n","\n","    #plt.savefig('../images/figure_6_4.png')\n","    plt.show()\n","    plt.close()\n","\n","    # display optimal policy\n","    print('Sarsa Optimal Policy:')\n","    print_optimal_policy(q_sarsa)\n","    print('Q-Learning Optimal Policy:')\n","    print_optimal_policy(q_q_learning)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|██████████| 50/50 [00:49<00:00,  1.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Sarsa Optimal Policy:\n","['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n","['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'R', 'D']\n","['U', 'U', 'R', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'R', 'D']\n","['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n","Q-Learning Optimal Policy:\n","['U', 'R', 'R', 'R', 'R', 'R', 'U', 'D', 'D', 'D', 'D', 'D']\n","['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D', 'R', 'D', 'D']\n","['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'D']\n","['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'G']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prioritized sweeping V1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuuIGsrrzbPi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from matplotlib.table import Table\n",
        "import heapq\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQDjY23jVt3G"
      },
      "source": [
        "# la clase Maze que contiene la configuracion del laberinto. Es facilmente modificable para cambiar el espacio, los obstaculos y estados inciales y finales\n",
        "\n",
        "# A wrapper class for a maze, containing all the information about the maze.\n",
        "# Basically it's initialized to DynaMaze by default, however it can be easily adapted\n",
        "# to other maze\n",
        "class Maze:\n",
        "    def __init__(self):\n",
        "        # maze width\n",
        "        self.WORLD_WIDTH = 9\n",
        "\n",
        "        # maze height\n",
        "        self.WORLD_HEIGHT = 6\n",
        "\n",
        "        # all possible actions\n",
        "        self.ACTION_UP = 0\n",
        "        self.ACTION_DOWN = 1\n",
        "        self.ACTION_LEFT = 2\n",
        "        self.ACTION_RIGHT = 3\n",
        "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
        "\n",
        "        self.ACTIONS_FIGS=['↑', '↓', '←', '→']\n",
        "\n",
        "\n",
        "        # start state\n",
        "        self.START_STATE = [2, 0]\n",
        "\n",
        "        # goal state\n",
        "        self.GOAL_STATES = [[0, 8]]\n",
        "\n",
        "        # all obstacles\n",
        "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
        "        self.old_obstacles = None\n",
        "        self.new_obstacles = None\n",
        "\n",
        "        # time to change obstacles\n",
        "        self.obstacle_switch_time = None\n",
        "\n",
        "        # initial state action pair values\n",
        "        # self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n",
        "\n",
        "        # the size of q value\n",
        "        self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))\n",
        "\n",
        "        # max steps\n",
        "        self.max_steps = float('inf')\n",
        "\n",
        "        # track the resolution for this maze\n",
        "        self.resolution = 1\n",
        "\n",
        "    # extend a state to a higher resolution maze\n",
        "    # @state: state in lower resoultion maze\n",
        "    # @factor: extension factor, one state will become factor^2 states after extension\n",
        "    def extend_state(self, state, factor):\n",
        "        new_state = [state[0] * factor, state[1] * factor]\n",
        "        new_states = []\n",
        "        for i in range(0, factor):\n",
        "            for j in range(0, factor):\n",
        "                new_states.append([new_state[0] + i, new_state[1] + j])\n",
        "        return new_states\n",
        "\n",
        "    # extend a state into higher resolution\n",
        "    # one state in original maze will become @factor^2 states in @return new maze\n",
        "    def extend_maze(self, factor):\n",
        "        new_maze = Maze()\n",
        "        new_maze.WORLD_WIDTH = self.WORLD_WIDTH * factor\n",
        "        new_maze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor\n",
        "        new_maze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor]\n",
        "        new_maze.GOAL_STATES = self.extend_state(self.GOAL_STATES[0], factor)\n",
        "        new_maze.obstacles = []\n",
        "        for state in self.obstacles:\n",
        "            new_maze.obstacles.extend(self.extend_state(state, factor))\n",
        "        new_maze.q_size = (new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions))\n",
        "       # new_maze.stateActionValues = np.zeros((new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions)))\n",
        "        new_maze.resolution = factor\n",
        "        return new_maze\n",
        "\n",
        "    # take @action in @state\n",
        "    # @return: [new state, reward]\n",
        "    def step(self, state, action):\n",
        "        x, y = state\n",
        "        if action == self.ACTION_UP:\n",
        "            x = max(x - 1, 0)\n",
        "        elif action == self.ACTION_DOWN:\n",
        "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
        "        elif action == self.ACTION_LEFT:\n",
        "            y = max(y - 1, 0)\n",
        "        elif action == self.ACTION_RIGHT:\n",
        "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
        "        if [x, y] in self.obstacles:\n",
        "            x, y = state\n",
        "        if [x, y] in self.GOAL_STATES:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "        return [x, y], reward"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBe6vQ8XVxGJ"
      },
      "source": [
        "# Parametros del algoritmo\n",
        "# a wrapper class for parameters of dyna algorithms\n",
        "class DynaParams:\n",
        "    def __init__(self):\n",
        "        # discount\n",
        "        self.gamma = 0.95\n",
        "\n",
        "        # probability for exploration\n",
        "        self.epsilon = 0.1\n",
        "\n",
        "        # step size\n",
        "        self.alpha = 0.1\n",
        "\n",
        "        # weight for elapsed time\n",
        "        self.time_weight = 0\n",
        "\n",
        "        # n-step planning\n",
        "        self.planning_steps = 5\n",
        "\n",
        "        # average over several independent runs\n",
        "        self.runs = 10\n",
        "\n",
        "        # algorithm names\n",
        "        self.methods = ['Dyna-Q', 'Dyna-Q+']\n",
        "\n",
        "        # threshold for priority queue\n",
        "        self.theta = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncTe-BB0Wck"
      },
      "source": [
        "# Trivial model for planning in Dyna-Q\n",
        "class TrivialModel:\n",
        "    # @rand: an instance of np.random.RandomState for sampling\n",
        "    def __init__(self, rand=np.random):\n",
        "        self.model = dict()\n",
        "        self.rand = rand\n",
        "\n",
        "    # feed the model with previous experience\n",
        "    def feed(self, state, action, next_state, reward):\n",
        "        state = deepcopy(state)\n",
        "        next_state = deepcopy(next_state)\n",
        "        if tuple(state) not in self.model.keys():\n",
        "            self.model[tuple(state)] = dict()\n",
        "        self.model[tuple(state)][action] = [list(next_state), reward]\n",
        "\n",
        "    # randomly sample from previous experience\n",
        "    def sample(self):\n",
        "        state_index = self.rand.choice(range(len(self.model.keys())))\n",
        "        state = list(self.model)[state_index]\n",
        "        action_index = self.rand.choice(range(len(self.model[state].keys())))\n",
        "        action = list(self.model[state])[action_index]\n",
        "        next_state, reward = self.model[state][action]\n",
        "        state = deepcopy(state)\n",
        "        next_state = deepcopy(next_state)\n",
        "        return list(state), action, list(next_state), reward"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP6YLY9zzmwK"
      },
      "source": [
        "# Model containing a priority queue for Prioritized Sweeping\n",
        "class PriorityModel(TrivialModel):\n",
        "    def __init__(self, rand=np.random):\n",
        "        TrivialModel.__init__(self, rand)\n",
        "        # maintain a priority queue\n",
        "        self.priority_queue = PriorityQueue()\n",
        "        # track predecessors for every state\n",
        "        self.predecessors = dict()\n",
        "\n",
        "    # add a @state-@action pair into the priority queue with priority @priority\n",
        "    def insert(self, priority, state, action):\n",
        "        # note the priority queue is a minimum heap, so we use -priority\n",
        "        self.priority_queue.add_item((tuple(state), action), -priority)\n",
        "\n",
        "    # @return: whether the priority queue is empty\n",
        "    def empty(self):\n",
        "        return self.priority_queue.empty()\n",
        "\n",
        "    # get the first item in the priority queue\n",
        "    def sample(self):\n",
        "        (state, action), priority = self.priority_queue.pop_item()\n",
        "        next_state, reward = self.model[state][action]\n",
        "        state = deepcopy(state)\n",
        "        next_state = deepcopy(next_state)\n",
        "        return -priority, list(state), action, list(next_state), reward\n",
        "\n",
        "    # feed the model with previous experience\n",
        "    def feed(self, state, action, next_state, reward):\n",
        "        state = deepcopy(state)\n",
        "        next_state = deepcopy(next_state)\n",
        "        TrivialModel.feed(self, state, action, next_state, reward)\n",
        "        if tuple(next_state) not in self.predecessors.keys():\n",
        "            self.predecessors[tuple(next_state)] = set()\n",
        "        self.predecessors[tuple(next_state)].add((tuple(state), action))\n",
        "\n",
        "    # get all seen predecessors of a state @state\n",
        "    def predecessor(self, state):\n",
        "        if tuple(state) not in self.predecessors.keys():\n",
        "            return []\n",
        "        predecessors = []\n",
        "        for state_pre, action_pre in list(self.predecessors[tuple(state)]):\n",
        "            predecessors.append([list(state_pre), action_pre, self.model[state_pre][action_pre][1]])\n",
        "        return predecessors\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0JFEjqG1SI3"
      },
      "source": [
        "class PriorityQueue:\n",
        "    def __init__(self):\n",
        "        self.pq = []\n",
        "        self.entry_finder = {}\n",
        "        self.REMOVED = '<removed-task>'\n",
        "        self.counter = 0\n",
        "\n",
        "    def add_item(self, item, priority=0):\n",
        "        if item in self.entry_finder:\n",
        "            self.remove_item(item)\n",
        "        entry = [priority, self.counter, item]\n",
        "        self.counter += 1\n",
        "        self.entry_finder[item] = entry\n",
        "        heapq.heappush(self.pq, entry)\n",
        "\n",
        "    def remove_item(self, item):\n",
        "        entry = self.entry_finder.pop(item)\n",
        "        entry[-1] = self.REMOVED\n",
        "\n",
        "    def pop_item(self):\n",
        "        while self.pq:\n",
        "            priority, count, item = heapq.heappop(self.pq)\n",
        "            if item is not self.REMOVED:\n",
        "                del self.entry_finder[item]\n",
        "                return item, priority\n",
        "        raise KeyError('pop from an empty priority queue')\n",
        "\n",
        "    def empty(self):\n",
        "        return not self.entry_finder"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vROXLsD91ZiS"
      },
      "source": [
        "def choose_action(state, q_value, maze, dyna_params):\n",
        "    if np.random.binomial(1, dyna_params.epsilon) == 1:\n",
        "        return np.random.choice(maze.actions)\n",
        "    else:\n",
        "        values = q_value[state[0], state[1], :]\n",
        "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO7KtusW1IDo"
      },
      "source": [
        "def dyna_q(q_value, model, maze, dyna_params):\n",
        "    state = maze.START_STATE\n",
        "    steps = 0\n",
        "    while state not in maze.GOAL_STATES:\n",
        "        # track the steps\n",
        "        steps += 1\n",
        "\n",
        "        # get action\n",
        "        action = choose_action(state, q_value, maze, dyna_params)\n",
        "\n",
        "        # take action\n",
        "        next_state, reward = maze.step(state, action)\n",
        "\n",
        "        # Q-Learning update\n",
        "        q_value[state[0], state[1], action] += \\\n",
        "            dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
        "                                 q_value[state[0], state[1], action])\n",
        "\n",
        "        # feed the model with experience\n",
        "        model.feed(state, action, next_state, reward)\n",
        "\n",
        "        # sample experience from the model\n",
        "        for t in range(0, dyna_params.planning_steps):\n",
        "            state_, action_, next_state_, reward_ = model.sample()\n",
        "            q_value[state_[0], state_[1], action_] += \\\n",
        "                dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) -\n",
        "                                     q_value[state_[0], state_[1], action_])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # check whether it has exceeded the step limit\n",
        "        if steps > maze.max_steps:\n",
        "            break\n",
        "\n",
        "    return steps"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic-qUje11Byf"
      },
      "source": [
        "def prioritized_sweeping(q_value, model, maze, dyna_params):\n",
        "    state = maze.START_STATE\n",
        "\n",
        "    # track the steps in this episode\n",
        "    steps = 0\n",
        "\n",
        "    # track the backups in planning phase\n",
        "    backups = 0\n",
        "\n",
        "    while state not in maze.GOAL_STATES:\n",
        "        steps += 1\n",
        "\n",
        "        # get action\n",
        "        action = choose_action(state, q_value, maze, dyna_params)\n",
        "\n",
        "        # take action\n",
        "        next_state, reward = maze.step(state, action)\n",
        "\n",
        "        # feed the model with experience\n",
        "        model.feed(state, action, next_state, reward)\n",
        "\n",
        "        # get the priority for current state action pair\n",
        "        priority = np.abs(reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
        "                          q_value[state[0], state[1], action])\n",
        "\n",
        "        if priority > dyna_params.theta:\n",
        "            model.insert(priority, state, action)\n",
        "\n",
        "        # start planning\n",
        "        planning_step = 0\n",
        "\n",
        "        # planning for several steps,\n",
        "        # although keep planning until the priority queue becomes empty will converge much faster\n",
        "        while planning_step < dyna_params.planning_steps and not model.empty():\n",
        "            # get a sample with highest priority from the model\n",
        "            priority, state_, action_, next_state_, reward_ = model.sample()\n",
        "\n",
        "            # update the state action value for the sample\n",
        "            delta = reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) - \\\n",
        "                    q_value[state_[0], state_[1], action_]\n",
        "            q_value[state_[0], state_[1], action_] += dyna_params.alpha * delta\n",
        "\n",
        "            # deal with all the predecessors of the sample state\n",
        "            for state_pre, action_pre, reward_pre in model.predecessor(state_):\n",
        "                priority = np.abs(reward_pre + dyna_params.gamma * np.max(q_value[state_[0], state_[1], :]) -\n",
        "                                  q_value[state_pre[0], state_pre[1], action_pre])\n",
        "                if priority > dyna_params.theta:\n",
        "                    model.insert(priority, state_pre, action_pre)\n",
        "            planning_step += 1\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # update the # of backups\n",
        "        backups += planning_step + 1\n",
        "\n",
        "    return backups\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vb8CYpg11M6"
      },
      "source": [
        "def check_path(q_values, maze):\n",
        "\n",
        "    max_steps = 14 * maze.resolution * 1.2\n",
        "    state = maze.START_STATE\n",
        "    steps = 0\n",
        "    while state not in maze.GOAL_STATES:\n",
        "        action = np.argmax(q_values[state[0], state[1], :])\n",
        "        state, _ = maze.step(state, action)\n",
        "        steps += 1\n",
        "        if steps > max_steps:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn6PkdJtzmtc",
        "outputId": "040df56b-3366-4d8f-8b26-4323af277923"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # get the original 6 * 9 maze\n",
        "    original_maze = Maze()\n",
        "\n",
        "    # set up the parameters for each algorithm\n",
        "    params_dyna = DynaParams()\n",
        "    params_dyna.planning_steps = 5\n",
        "    params_dyna.alpha = 0.5\n",
        "    params_dyna.gamma = 0.95\n",
        "\n",
        "    params_prioritized = DynaParams()\n",
        "    params_prioritized.theta = 0.0001\n",
        "    params_prioritized.planning_steps = 5\n",
        "    params_prioritized.alpha = 0.5\n",
        "    params_prioritized.gamma = 0.95\n",
        "\n",
        "    params = [params_prioritized, params_dyna]\n",
        "\n",
        "    # set up models for planning\n",
        "    models = [PriorityModel, TrivialModel]\n",
        "    method_names = ['Prioritized Sweeping', 'Dyna-Q']\n",
        "\n",
        "    # due to limitation of my machine, I can only perform experiments for 5 mazes\n",
        "    # assuming the 1st maze has w * h states, then k-th maze has w * h * k * k states\n",
        "    num_of_mazes = 5\n",
        "\n",
        "    # build all the mazes\n",
        "    mazes = [original_maze.extend_maze(i) for i in range(1, num_of_mazes + 1)]\n",
        "    methods = [prioritized_sweeping, dyna_q]\n",
        "\n",
        "    # My machine cannot afford too many runs...\n",
        "    runs = 5\n",
        "\n",
        "    # track the # of backups\n",
        "    backups = np.zeros((runs, 2, num_of_mazes))\n",
        "\n",
        "    for run in range(0, runs):\n",
        "        for i in range(0, len(method_names)):\n",
        "            for mazeIndex, maze in zip(range(0, len(mazes)), mazes):\n",
        "                print('run %d, %s, maze size %d' % (run, method_names[i], maze.WORLD_HEIGHT * maze.WORLD_WIDTH))\n",
        "\n",
        "                # initialize the state action values\n",
        "                q_value = np.zeros(maze.q_size)\n",
        "\n",
        "                # track steps / backups for each episode\n",
        "                steps = []\n",
        "\n",
        "                # generate the model\n",
        "                model = models[i]()\n",
        "\n",
        "                # play for an episode\n",
        "                while True:\n",
        "                    steps.append(methods[i](q_value, model, maze, params[i]))\n",
        "\n",
        "                    # print best actions w.r.t. current state-action values\n",
        "                    # printActions(currentStateActionValues, maze)\n",
        "\n",
        "                    # check whether the (relaxed) optimal path is found\n",
        "                    if check_path(q_value, maze):\n",
        "                        break\n",
        "\n",
        "                # update the total steps / backups for this maze\n",
        "                backups[run, i, mazeIndex] = np.sum(steps)\n",
        "\n",
        "    backups = backups.mean(axis=0)\n",
        "\n",
        "    # Dyna-Q performs several backups per step\n",
        "    backups[1, :] *= params_dyna.planning_steps + 1\n",
        "\n",
        "    for i in range(0, len(method_names)):\n",
        "        plt.plot(np.arange(1, num_of_mazes + 1), backups[i, :], label=method_names[i])\n",
        "    plt.xlabel('maze resolution factor')\n",
        "    plt.ylabel('backups until optimal solution')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "\n",
        "    #plt.savefig('../images/example_8_4.png')\n",
        "    plt.close()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run 0, Prioritized Sweeping, maze size 54\n",
            "run 0, Prioritized Sweeping, maze size 216\n",
            "run 0, Prioritized Sweeping, maze size 486\n",
            "run 0, Prioritized Sweeping, maze size 864\n",
            "run 0, Prioritized Sweeping, maze size 1350\n",
            "run 0, Dyna-Q, maze size 54\n",
            "run 0, Dyna-Q, maze size 216\n",
            "run 0, Dyna-Q, maze size 486\n",
            "run 0, Dyna-Q, maze size 864\n",
            "run 0, Dyna-Q, maze size 1350\n",
            "run 1, Prioritized Sweeping, maze size 54\n",
            "run 1, Prioritized Sweeping, maze size 216\n",
            "run 1, Prioritized Sweeping, maze size 486\n",
            "run 1, Prioritized Sweeping, maze size 864\n",
            "run 1, Prioritized Sweeping, maze size 1350\n",
            "run 1, Dyna-Q, maze size 54\n",
            "run 1, Dyna-Q, maze size 216\n",
            "run 1, Dyna-Q, maze size 486\n",
            "run 1, Dyna-Q, maze size 864\n",
            "run 1, Dyna-Q, maze size 1350\n",
            "run 2, Prioritized Sweeping, maze size 54\n",
            "run 2, Prioritized Sweeping, maze size 216\n",
            "run 2, Prioritized Sweeping, maze size 486\n",
            "run 2, Prioritized Sweeping, maze size 864\n",
            "run 2, Prioritized Sweeping, maze size 1350\n",
            "run 2, Dyna-Q, maze size 54\n",
            "run 2, Dyna-Q, maze size 216\n",
            "run 2, Dyna-Q, maze size 486\n",
            "run 2, Dyna-Q, maze size 864\n",
            "run 2, Dyna-Q, maze size 1350\n",
            "run 3, Prioritized Sweeping, maze size 54\n",
            "run 3, Prioritized Sweeping, maze size 216\n",
            "run 3, Prioritized Sweeping, maze size 486\n",
            "run 3, Prioritized Sweeping, maze size 864\n",
            "run 3, Prioritized Sweeping, maze size 1350\n",
            "run 3, Dyna-Q, maze size 54\n",
            "run 3, Dyna-Q, maze size 216\n",
            "run 3, Dyna-Q, maze size 486\n",
            "run 3, Dyna-Q, maze size 864\n",
            "run 3, Dyna-Q, maze size 1350\n",
            "run 4, Prioritized Sweeping, maze size 54\n",
            "run 4, Prioritized Sweeping, maze size 216\n",
            "run 4, Prioritized Sweeping, maze size 486\n",
            "run 4, Prioritized Sweeping, maze size 864\n",
            "run 4, Prioritized Sweeping, maze size 1350\n",
            "run 4, Dyna-Q, maze size 54\n",
            "run 4, Dyna-Q, maze size 216\n",
            "run 4, Dyna-Q, maze size 486\n",
            "run 4, Dyna-Q, maze size 864\n",
            "run 4, Dyna-Q, maze size 1350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tq7q_jhzmq2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6dA7jvXzmog"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_X-9lF3zml8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUxFr3onzmjk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1pnp1YAzmhT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3E8e0QYzmei"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otMDXLNVzmcA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
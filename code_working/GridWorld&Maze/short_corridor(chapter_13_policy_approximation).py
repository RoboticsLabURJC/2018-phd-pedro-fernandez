# -*- coding: utf-8 -*-
"""short_corridor(chapter 13 Policy approximation).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TW2kMf6wQMnlVNmiEeh2Kj9lkvoK91LA
"""

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from tqdm import tqdm

def true_value(p):
    """ True value of the first state
    Args:
        p (float): probability of the action 'right'.
    Returns:
        True value of the first state.
        The expression is obtained by manually solving the easy linear system
        of Bellman equations using known dynamics.
    """
    return (2 * p - 4) / (p * (1 - p))

#def softmax(x):
#    t = np.exp(x - np.max(x))
#    return t / np.sum(t)

if __name__ == '__main__':

    epsilon = 0.05
    fig, ax = plt.subplots(1, 1)

    # Plot a graph
    p = np.linspace(0.01, 0.99, 100)
    
    y = true_value(p)
    ax.plot(p, y, color='red')

    # Find a maximum point, can also be done analytically by taking a derivative
    imax = np.argmax(y)
    pmax = p[imax]
    ymax = y[imax]
    ax.plot(pmax, ymax, color='green', marker="*", label="optimal point: f({0:.2f}) = {1:.2f}".format(pmax, ymax))

    # Plot points of two epsilon-greedy policies
    ax.plot(epsilon, true_value(epsilon), color='magenta', marker="o", label="epsilon-greedy left")
    ax.plot(1 - epsilon, true_value(1 - epsilon), color='blue', marker="o", label="epsilon-greedy right")

    ax.set_ylabel("Value of the first state")
    ax.set_xlabel("Probability of the action 'right'")
    ax.set_title("Short corridor with switched actions")
    ax.set_ylim(ymin=-105.0, ymax=5)
    ax.legend()

    plt.show()
    #plt.savefig('../images/example_13_1.png')
    plt.close()

"""#=============="""

true_value(0.05)

q = np.linspace(0.01, 0.99, 100)
print(q)
print(q.dtype)
print(q.size)

h = (2 * q - 4) / (q * (1 - q))
print(h)
imax2 = np.argmax(h)
print(imax2)
print(p[imax2])
print(h[imax2])

xx = (2*0.59) - 4

xx2 = 0.59*(1-0.59)

xx/xx2